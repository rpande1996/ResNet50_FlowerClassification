{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet-50 Model Training for Flower Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HX8ykoQ4TI7W"
   },
   "source": [
    "## Imports\n",
    "\n",
    "Required imports for training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3hjhYARTG1N"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import tarfile\n",
    "import os\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nURBM7GgTS0k"
   },
   "source": [
    "## Raw dataset downloader functions \n",
    "\n",
    "Required functions for the initial dataset download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDresz2mTNOs"
   },
   "outputs": [],
   "source": [
    "def download(url: str, path: str) -> None:\n",
    "    \"\"\"\n",
    "    Function: Download files from a url to a specific directory\n",
    "    Parameters:\n",
    "        1. url: File url\n",
    "        2. path: Target directory\n",
    "    Returns:\n",
    "        1. None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except:\n",
    "        path = path\n",
    "    filename = os.path.basename(url)\n",
    "    urllib.request.urlretrieve(url, f\"{path}/{filename}\")\n",
    "\n",
    "\n",
    "def extract(path: str, target: str) -> None:\n",
    "    \"\"\"\n",
    "    Function: Extract tar files from a directory to a specific directory\n",
    "    Parameters:\n",
    "        1. path: .tar file directory\n",
    "        2. target: Target directory\n",
    "    Returns:\n",
    "        1. None\n",
    "    \"\"\"\n",
    "    file = tarfile.open(path)\n",
    "    file.extractall(target)\n",
    "    file.close()\n",
    "    os.remove(path)\n",
    "\n",
    "\n",
    "def dataset(ds_url: str, label_url: str, target: str) -> None:\n",
    "    \"\"\"\n",
    "    Function: Download raw dataset\n",
    "    Parameters:\n",
    "        1. ds_url: Raw data url\n",
    "        2. label_url: Label url\n",
    "        3. target: Target location for the dataset and label matrix\n",
    "    Returns:\n",
    "        1. None\n",
    "    \"\"\"\n",
    "    download(ds_url, target)\n",
    "    ds_name = os.path.basename(ds_url)\n",
    "    ds_loc = target + ds_name\n",
    "    extract(ds_loc, target)\n",
    "    download(label_url, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwl-tuZtTanE"
   },
   "source": [
    "## Image pre-processing functions\n",
    "\n",
    "Required functions the process the individual images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cr9ueic6Ti3e"
   },
   "outputs": [],
   "source": [
    "def crop(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function: Randomly crop the image into (224, 224) for the model\n",
    "    Parameters:\n",
    "        1. image: Image array\n",
    "    Returns:\n",
    "        1. Cropped image array\n",
    "    \"\"\"\n",
    "    crop_height, crop_width = 224, 224\n",
    "    max_x = image.shape[1] - crop_width\n",
    "    max_y = image.shape[0] - crop_height\n",
    "    x = np.random.randint(0, max_x)\n",
    "    y = np.random.randint(0, max_y)\n",
    "    crop = image[y: y + crop_height, x: x + crop_width]\n",
    "    return crop\n",
    "\n",
    "\n",
    "def resize(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function: Resize the image into (224, 224) for the model\n",
    "    Parameters:\n",
    "        1. image: Image array\n",
    "    Returns:\n",
    "        1. Resized image array\n",
    "    \"\"\"\n",
    "    dim = (224, 224)\n",
    "    resized = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)\n",
    "    return resized\n",
    "\n",
    "\n",
    "def orient(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function: Randomly flip the image along x-axis, y-axis or both\n",
    "    Parameters:\n",
    "        1. image: Image array\n",
    "    Returns:\n",
    "        1. Flipped image array\n",
    "    \"\"\"\n",
    "    return cv2.flip(image, random.choice([0, 1, -1]))\n",
    "\n",
    "\n",
    "def section(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function: Randomly resize or crop the image array\n",
    "    Parameters:\n",
    "        1. image: Image array\n",
    "    Returns:\n",
    "        1. Cropped/Resized image array\n",
    "    \"\"\"\n",
    "    functions = {\n",
    "        1: resize,\n",
    "        2: crop\n",
    "    }\n",
    "    return functions[random.choice([1, 2])](image)\n",
    "\n",
    "\n",
    "def normalize(matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function: Normalize image array\n",
    "    Parameters:\n",
    "        1. matrix: Image array\n",
    "    Return:\n",
    "        1. Normalized image array\n",
    "    \"\"\"\n",
    "    return ((matrix / 255.0) - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GS_I9scKToCc"
   },
   "source": [
    "## Dataset pre-processing functions\n",
    "\n",
    "Required functions to process the raw dataset into acceptable dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlMw3nJRTqnM"
   },
   "outputs": [],
   "source": [
    "def generateDistribution(labels: list[int]) -> dict[int, list[int]]:\n",
    "    \"\"\"\n",
    "    Function: Generate distribution of the data according to its labels\n",
    "    Parameters:\n",
    "        1. labels: List of image labels\n",
    "    Returns:\n",
    "        1. A dictionary of labels as keys and a list image numbers corresponding that label\n",
    "    \"\"\"\n",
    "    dist = {}\n",
    "    for l in range(len(labels)):\n",
    "        if labels[l] not in dist:\n",
    "            dist[labels[l]] = [l + 1]\n",
    "        else:\n",
    "            dist[labels[l]].append(l + 1)\n",
    "    return dist\n",
    "\n",
    "\n",
    "def generateDataSet(split: float, dist: dict[int, list[int]]) -> tuple[dict[int, list[int]], dict[int, list[int]]]:\n",
    "    \"\"\"\n",
    "    Function: Split dataset into test dataset and training dataset\n",
    "    Parameters:\n",
    "        1. split: Splitting ratio for training data\n",
    "        2. dist: Dictionary of labels as keys and a list image numbers corresponding that label\n",
    "    Returns:\n",
    "        1. Training data distribution dictionary\n",
    "        2. Testing data distribution dictionary\n",
    "    \"\"\"\n",
    "    trainingDS = {}\n",
    "    testingDS = {}\n",
    "    for i in range(len(list(dist.keys()))):\n",
    "        trainingDS[i + 1] = []\n",
    "        testingDS[i + 1] = []\n",
    "    for i in range(len(list(dist.keys()))):\n",
    "        img_nums = list(dist[i + 1])\n",
    "        temp = random.sample(img_nums, k=int(len(img_nums) * split))\n",
    "        trainingDS[i + 1] = temp\n",
    "        testingDS[i + 1] = [x for x in img_nums if x not in temp]\n",
    "    return trainingDS, testingDS\n",
    "\n",
    "\n",
    "def updateDataSet(datasetDict: dict[int, list[int]]) -> list[list[str, int]]:\n",
    "    \"\"\"\n",
    "    Function: Convert data distribution dictionary into a list of a list of image location and its respective label\n",
    "    Parameters:\n",
    "        1. datasetDict: Dictionary of labels as keys and a list image numbers corresponding that label\n",
    "    Returns:\n",
    "        1. List of a list of image location and its respective label\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for i in range(len(datasetDict.keys())):\n",
    "        for j in datasetDict[i + 1]:\n",
    "            image = './dataset/jpg/image_' + str(j).rjust(5, '0') + '.jpg'\n",
    "            dataset.append([image, i])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def shuffleDataSet(dataset: list[list[str, int]]) -> list[list[str, int]]:\n",
    "    \"\"\"\n",
    "    Function: Shuffle dataset\n",
    "    Parameters:\n",
    "        1. dataset: List of a list of image location and its respective label\n",
    "    Returns:\n",
    "        1. Shuffled dataset\n",
    "    \"\"\"\n",
    "    return random.sample(dataset, len(dataset))\n",
    "\n",
    "\n",
    "def imageTransformation(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function: Transform image array randomly\n",
    "    Parameters:\n",
    "        1. image: Image array\n",
    "    Returns:\n",
    "        1. Randomly flipped, cropped/resized and normalized image array\n",
    "    \"\"\"\n",
    "    return normalize(section(orient(image)))\n",
    "\n",
    "\n",
    "def updateLabels(labels: np.ndarray, n_classes: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function: Convert list of labels into a 3D array of size (length of list of labels, total unique number of labels) as per the label probability\n",
    "    Parameters:\n",
    "        1. labels: 1D array of labels\n",
    "        2. n_classes: total unique number of labels\n",
    "    Returns:\n",
    "        1. Array of labels\n",
    "    \"\"\"\n",
    "    labels = list(labels)\n",
    "    labelArr = np.zeros((len(labels), n_classes))\n",
    "    for i, j in enumerate(labels):\n",
    "        labelArr[i][j] = 1\n",
    "    return labelArr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kKDAg5qTspF"
   },
   "source": [
    "## Utility functions\n",
    "\n",
    "Required functions for miscellaneous uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "heMWaJIqTwr0"
   },
   "outputs": [],
   "source": [
    "def accuracy(correctList: list[int], totalList: list[int]) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Function: Calculate the number of correct predictions and total number of predictions\n",
    "    Parameters:\n",
    "        1. correctList: List of correct predictions for each label\n",
    "        2. totalList: List of total predictions for each label\n",
    "    Returns:\n",
    "        1. Number of correct predictions\n",
    "        2. Number of total predictions\n",
    "    \"\"\"\n",
    "    correctList = list(correctList.cpu().numpy())\n",
    "    totalList = list(totalList.cpu().numpy())\n",
    "    correct = 0\n",
    "    total = len(correctList)\n",
    "    if correctList != totalList:\n",
    "        for i in range(len(correctList)):\n",
    "            if correctList[i] == totalList[i]:\n",
    "                correct += 1\n",
    "        return correct, total\n",
    "    else:\n",
    "        return total, total\n",
    "\n",
    "\n",
    "def plotAccuracies(correct: list[int], total: list[int], names: list[str], sample: int) -> None:\n",
    "    \"\"\"\n",
    "    Function: Plot a specific number of most accurate and least accurate examples\n",
    "    Parameters:\n",
    "        1. correct: List of correct predictions for each label\n",
    "        2. total: List of total predictions for each label\n",
    "        3. names: List of names for corresponding labels\n",
    "        4. sample: Number of samples to be displayed\n",
    "    Returns:\n",
    "        1. None\n",
    "    \"\"\"\n",
    "    if sample > len(np.unique([a / b for a, b in zip(correct, total)])) // 2:\n",
    "        sample = len(np.unique([a / b for a, b in zip(correct, total)])) // 2\n",
    "        print(\n",
    "            f'Changing sample size to maximum allowed value. If you want to reduce sample size, please choose between [1, {sample}]')\n",
    "    yLab = ['Most Accurate', 'Least Accurate']\n",
    "    # Calculating the percentage of accuracy for each label\n",
    "    percent = [a / b for a, b in zip(correct, total)]\n",
    "    # Sorting the list to get the most accurate and least accurate examples\n",
    "    sorted_list = percent.copy()\n",
    "    sorted_list.sort()\n",
    "    sorted_list.reverse()\n",
    "    disp_names = []\n",
    "    disp_vals = []\n",
    "    colors = ('palegreen', 'salmon')\n",
    "    # Getting the labels and accuracy percentages\n",
    "    for x in range(len(sorted_list)):\n",
    "        for y in range(len(percent)):\n",
    "            if percent[y] == sorted_list[x]:\n",
    "                if names[y] not in disp_names:\n",
    "                    if sorted_list[x] not in disp_vals:\n",
    "                        disp_vals.append(sorted_list[x])\n",
    "                        disp_names.append(names[y])\n",
    "    act_vals = disp_vals.copy()\n",
    "    # Dividing the list into 2 parts; most and least accurate for the given number (sample) of examples\n",
    "    disp_vals = [disp_vals[:sample], list(np.flip(disp_vals[sample:]))]\n",
    "    disp_names = [disp_names[:sample], list(np.flip(disp_names[sample:]))]\n",
    "    # Plotting the pie chart of the respective accuracies\n",
    "    if sample > 0:\n",
    "        for i in range(2 * sample):\n",
    "            row = i // sample\n",
    "            col = i - (row * sample)\n",
    "            ax1 = plt.subplot2grid((2, sample), (row, col))\n",
    "            plt.pie([disp_vals[row][col], 1.0 - disp_vals[row][col]], colors=colors)\n",
    "            if col == 0:\n",
    "                plt.ylabel(f'{yLab[row]}')\n",
    "            plt.text(0, -1.25, f'{round(disp_vals[row][col] * 100, 2)}% Accuracy', ha='center', wrap=True)\n",
    "            plt.title(disp_names[row][col])\n",
    "        plt.savefig('./output/test.png')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\n",
    "            f'Changing sample size to minimum allowed value. If you want to increase sample size, please choose sample size between [1,{int(len((act_vals)) / 2)}]')\n",
    "        plotAccuracies(correct, total, names, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyMhOmPVTz00"
   },
   "source": [
    "## Model training functions\n",
    "\n",
    "Main ```train``` and ```test``` functions for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6MDVmW_T3xT"
   },
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module, optim: torch.optim.Optimizer, loss_fn: torch.nn.Module,\n",
    "          trainingDS: dict[int, list[int]],\n",
    "          epochs: int = 300, batch_size: int = 8) -> tuple[list[float], list[float], list[float], list[float]]:\n",
    "    \"\"\"\n",
    "    Function: Train the input model for specific number of epochs (with a specific batch size)\n",
    "    Parameters:\n",
    "        1. model: Machine learning model\n",
    "        2. optim: Model optimizer\n",
    "        3. loss_fn: Loss function\n",
    "        4. trainingDS: Training dataset for the model\n",
    "        5. epochs: Number of epochs\n",
    "        6. batch_size: Batch size\n",
    "    Returns:\n",
    "        1. List of accuracies for the training loop for each epoch\n",
    "        2. List of accuracies for the validation loop for each epoch\n",
    "        3. List of losses for the validation loop for each epoch\n",
    "        4. List of losses for the training loop for each epoch\n",
    "    \"\"\"\n",
    "    trainingAccuracy = []\n",
    "    validationAccuracy = []\n",
    "    validationLosses = []\n",
    "    trainingLosses = []\n",
    "    minimumAccuracy = 0.5\n",
    "    for epoch in range(epochs):\n",
    "        ## Training Loop\n",
    "        # Splitting the data into training and validation sets\n",
    "        trainingDS_split, validationDS_split = generateDataSet(0.7 / 0.9, trainingDS)\n",
    "        validationLosses_value = 0.0\n",
    "        trainingLosses_value = 0.0\n",
    "        # Shuffling the training dataset\n",
    "        shuffledTrainingDS = shuffleDataSet(updateDataSet(trainingDS_split))\n",
    "        trainingTotal = 0\n",
    "        trainingCorrect = 0\n",
    "        validationCorrect = 0\n",
    "        validationTotal = 0\n",
    "        if len(shuffledTrainingDS) % batch_size == 0:\n",
    "            trainEnd = int(len(shuffledTrainingDS) / batch_size)\n",
    "        else:\n",
    "            trainEnd = int(len(shuffledTrainingDS) / batch_size) + 1\n",
    "        for i in tqdm(range(trainEnd)):\n",
    "            # Dividing the dataset into batches of the given batch size\n",
    "            trainingBatches = np.asarray(shuffledTrainingDS[i * batch_size:(i + 1) * batch_size]).T[0]\n",
    "            # Transforming the dataset for accurate training\n",
    "            trainingBatches = np.asarray(\n",
    "                [imageTransformation(cv2.imread(x)).astype(np.float32) for x in trainingBatches])\n",
    "            trainingBatches = torch.tensor(trainingBatches)\n",
    "            trainingBatches = torch.permute(trainingBatches, (0, 3, 1, 2)).to(device)\n",
    "            # Extracting the actual labels for the above dataset\n",
    "            trainingLabels = np.asarray(shuffledTrainingDS[i * batch_size:(i + 1) * batch_size]).T[1].astype(np.int64)\n",
    "            trainingLabels = updateLabels(trainingLabels, 102)\n",
    "            trainingLabels = torch.tensor(trainingLabels).to(device)\n",
    "            trainingLabels_output = model(trainingBatches)\n",
    "            _, trainingLabels_actual = torch.max(trainingLabels, 1)\n",
    "            _, trainingLabels_predicted = torch.max(trainingLabels_output, 1)\n",
    "            # Calculating the training accuracy for current batch\n",
    "            trainingCorrect_i, trainingTotal_i = accuracy(trainingLabels_actual, trainingLabels_predicted)\n",
    "            trainingCorrect += trainingCorrect_i\n",
    "            trainingTotal += trainingTotal_i\n",
    "            # Calculating the training loss for current batch\n",
    "            trainingLoss = loss_fn(trainingLabels_output, trainingLabels)\n",
    "            trainingLosses_value += batch_size * trainingLoss.item()\n",
    "            optim.zero_grad()\n",
    "            trainingLoss.backward()\n",
    "            optim.step()\n",
    "        print(\n",
    "            f'Training Accuracy at {epoch + 1} epoch/s: {round((trainingCorrect / trainingTotal) * 100, 2)} % - ({trainingCorrect}/{trainingTotal})')\n",
    "        # Calculating the training accuracy & training loss for current epoch\n",
    "        trainingAccuracy.append(round((trainingCorrect / trainingTotal) * 100, 2))\n",
    "        trainingLosses.append(trainingLosses_value / len(shuffledTrainingDS))\n",
    "        print(f'Training Loss at {epoch + 1} epoch/s: {trainingLosses_value / (len(shuffledTrainingDS))}')\n",
    "        ## Validation Loop\n",
    "        with torch.no_grad():\n",
    "            # Shuffling the validation dataset\n",
    "            shuffledValidationDS = shuffleDataSet(updateDataSet(validationDS_split))\n",
    "            if len(shuffledValidationDS) % batch_size == 0:\n",
    "                valEnd = int(len(shuffledValidationDS) / batch_size)\n",
    "            else:\n",
    "                valEnd = int(len(shuffledValidationDS) / batch_size) + 1\n",
    "            for j in tqdm(range(valEnd)):\n",
    "                # Dividing the dataset into batches of the given batch size\n",
    "                validationBatches = torch.tensor(np.asarray([resize(cv2.imread(x)).astype(np.float32) for x in\n",
    "                                                             np.asarray(shuffledValidationDS[\n",
    "                                                                        j * batch_size:(j + 1) * batch_size]).T[0]]))\n",
    "                validationBatches = torch.permute(validationBatches, (0, 3, 1, 2)).to(device)\n",
    "                # Extracting the actual labels for the above dataset\n",
    "                validationLabels = np.asarray(shuffledValidationDS[j * batch_size:(j + 1) * batch_size]).T[1].astype(\n",
    "                    np.int64)\n",
    "                validationLabels = updateLabels(validationLabels, 102)\n",
    "                validationLabels = torch.tensor(validationLabels).to(device)\n",
    "                validationLabels_output = model(validationBatches)\n",
    "                # Calculating the validation loss for current batch\n",
    "                validationLoss = loss_fn(validationLabels_output, validationLabels)\n",
    "                validationLosses_value += batch_size * validationLoss.item()\n",
    "                _, validationLabels_actual = torch.max(validationLabels, 1)\n",
    "                _, validationLabels_predicted = torch.max(validationLabels_output, 1)\n",
    "                # Calculating the validation accuracy for current batch\n",
    "                validationCorrect_i, validationTotal_i = accuracy(validationLabels_actual, validationLabels_predicted)\n",
    "                validationCorrect += validationCorrect_i\n",
    "                validationTotal += validationTotal_i\n",
    "            # Saving the most accurate model\n",
    "            if validationCorrect / validationTotal > minimumAccuracy:\n",
    "                minimumAccuracy = validationCorrect / validationTotal\n",
    "                if not os.path.exists('./models/'):\n",
    "                    os.makedirs('./models/')\n",
    "                torch.save(model, './models/final.pt')\n",
    "        # Calculating the validation accuracy & validation loss for current epoch\n",
    "        validationLosses.append(validationLosses_value / (len(shuffledValidationDS)))\n",
    "        print(f'Validation Loss at {epoch + 1} epoch/s: {validationLosses_value / (len(shuffledValidationDS))}')\n",
    "        print(\n",
    "            f'Validation Accuracy at {epoch + 1} epoch/s: {round((validationCorrect / validationTotal) * 100, 2)} % - ({validationCorrect}/{validationTotal})')\n",
    "        validationAccuracy.append(round((validationCorrect / validationTotal) * 100, 2))\n",
    "    # Retraining the model if maximum accuracy is not reached\n",
    "    if max(validationAccuracy) <= 74.0:\n",
    "        print('\\n\\nIncreased training!!!')\n",
    "        trainingAccuracy_new, validationAccuracy_new, validationLosses_new, trainingLosses_new = train(model, optim,\n",
    "                                                                                                       loss_fn,\n",
    "                                                                                                       trainingDS,\n",
    "                                                                                                       epochs=50)\n",
    "        trainingAccuracy.extend(trainingAccuracy_new)\n",
    "        validationAccuracy.extend(validationAccuracy_new)\n",
    "        validationLosses.extend(validationLosses_new)\n",
    "        trainingLosses.extend(trainingLosses_new)\n",
    "    return trainingAccuracy, validationAccuracy, validationLosses, trainingLosses\n",
    "\n",
    "\n",
    "def test(model: torch.nn.Module, testingDS: dict[int, list[int]], batch_size: int = 8) -> tuple[list[int], list[int]]:\n",
    "    \"\"\"\n",
    "    Function: Test the trained model with a specific batch size\n",
    "    Parameters:\n",
    "        1. model: Machine learning model\n",
    "        2. testingDS: Testing dataset for the model\n",
    "        3. batch_size: Batch size\n",
    "    Returns:\n",
    "        1. List of correct predictions\n",
    "        2. List of total predictions\n",
    "    \"\"\"\n",
    "    labels = list(range(102))\n",
    "    correctPredicted = {classVal: 0 for classVal in labels}\n",
    "    totalPredicted = {classVal: 0 for classVal in labels}\n",
    "    with torch.no_grad():\n",
    "        # Shuffling the testing dataset\n",
    "        shuffledTestingDS = shuffleDataSet(updateDataSet(testingDS))\n",
    "        if len(shuffledTestingDS) % batch_size == 0:\n",
    "            testEnd = int(len(shuffledTestingDS) / batch_size)\n",
    "        else:\n",
    "            testEnd = int(len(shuffledTestingDS) / batch_size) + 1\n",
    "        for j in tqdm(range(testEnd)):\n",
    "            ## Testing Loop\n",
    "            # Dividing the dataset into batches of the given batch size\n",
    "            testingBatches = torch.tensor(np.asarray([normalize(resize(cv2.imread(x))).astype(np.float32) for x in\n",
    "                                                      np.asarray(\n",
    "                                                          shuffledTestingDS[j * batch_size:(j + 1) * batch_size]).T[\n",
    "                                                          0]]))\n",
    "            testingBatches = torch.permute(testingBatches, (0, 3, 1, 2)).to(device)\n",
    "            # Extracting the actual labels for the above dataset\n",
    "            testingLabels = np.asarray(shuffledTestingDS[j * batch_size:(j + 1) * batch_size]).T[1].astype(np.int64)\n",
    "            testingLabels = torch.tensor(testingLabels).to(device)\n",
    "            testingLabels_output = model(testingBatches)\n",
    "            _, testingLabels_predicted = torch.max(testingLabels_output, 1)\n",
    "            # Calculating the accurately predicted labels\n",
    "            for l, p in zip(testingLabels, testingLabels_predicted):\n",
    "                if l == p:\n",
    "                    correctPredicted[labels[l]] += 1\n",
    "                totalPredicted[labels[l]] += 1\n",
    "    torch.save(model, './models/final.pt')\n",
    "    print(\n",
    "        f'Model Accuracy: {round(100 * (sum(list(correctPredicted.values())) / sum(list(totalPredicted.values()))), 2)}%')\n",
    "    return list(correctPredicted.values()), list(totalPredicted.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drh4c02pWUEa"
   },
   "source": [
    "## Model inference function\n",
    "\n",
    "Main function for inference of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izB-gHIdWTly"
   },
   "outputs": [],
   "source": [
    "# Model inference function\n",
    "\n",
    "def infer(image: np.ndarray, labels: dict[int, str]) -> str:\n",
    "    \"\"\"\n",
    "    Function: Predict the flower name of input image using the trained model\n",
    "    Parameters:\n",
    "        1. image: Image array\n",
    "        2. labels: Dictionary of flower names to their respective labels\n",
    "    Returns:\n",
    "        1. Model predicted flower name\n",
    "    \"\"\"\n",
    "\n",
    "    model = torch.load('./models/final.pt')\n",
    "    model = model.eval().to(device)\n",
    "    # Pre-processing the input image for the model\n",
    "    image = (resize(image) / 255.0).astype(np.float32)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = torch.tensor(image)\n",
    "    image = torch.permute(image, (0, 3, 1, 2)).to(device)\n",
    "    # Using the model to predict the flower type of input image\n",
    "    with torch.no_grad():\n",
    "        out = model(image)\n",
    "        _, lb_out = torch.max(out, 1)\n",
    "        lb_out = list(lb_out.cpu().numpy())[0]\n",
    "    flowerName = labels[lb_out]\n",
    "    return flowerName, lb_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8leIvotVQ4V"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Downloads the raw dataset and initialises the flower names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBLLnPimT64s"
   },
   "outputs": [],
   "source": [
    "# Raw dataset downloading\n",
    "\n",
    "dataset('https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz',\n",
    "        'https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat',\n",
    "        './dataset/')\n",
    "\n",
    "# Flower Names:\n",
    "labels = {\n",
    "    0: 'Pink Primrose',\n",
    "    1: 'Hard-Leaved Pocket Orchid',\n",
    "    2: 'Canterbury Bells',\n",
    "    3: 'Sweet Pea',\n",
    "    4: 'English Marigold',\n",
    "    5: 'Tiger Lily',\n",
    "    6: 'Moon Orchid',\n",
    "    7: 'Bird of Paradise',\n",
    "    8: 'Monkshood',\n",
    "    9: 'Globe Thistle',\n",
    "    10: 'Snapdragon',\n",
    "    11: 'Colts Foot',\n",
    "    12: 'King Protea',\n",
    "    13: 'Spear Thistle',\n",
    "    14: 'Yellow Iris',\n",
    "    15: 'Globe-Flower',\n",
    "    16: 'Purple Coneflower',\n",
    "    17: 'Peruvian Lily',\n",
    "    18: 'Balloon Flower',\n",
    "    19: 'Giant White Arum Lily',\n",
    "    20: 'Fire Lily',\n",
    "    21: 'Pincushion Flower',\n",
    "    22: 'Fritillary',\n",
    "    23: 'Red Ginger',\n",
    "    24: 'Grape Hyacinth',\n",
    "    25: 'Corn Poppy',\n",
    "    26: 'Prince of Wales Feathers',\n",
    "    27: 'Stemless Gentian',\n",
    "    28: 'Artichoke',\n",
    "    29: 'Sweet William',\n",
    "    30: 'Carnation',\n",
    "    31: 'Garden Phlox',\n",
    "    32: 'Love in the Mist',\n",
    "    33: 'Mexican Aster',\n",
    "    34: 'Sea Holly',\n",
    "    35: 'Ruby-Lipped Cattleya',\n",
    "    36: 'Cape Flower',\n",
    "    37: 'Masterwort',\n",
    "    38: 'Siam Tulip',\n",
    "    39: 'Lenten Rose',\n",
    "    40: 'Barbeton Daisy',\n",
    "    41: 'Daffodil',\n",
    "    42: 'Sword Lily',\n",
    "    43: 'Poinsettia',\n",
    "    44: 'Bolero Deep Blue',\n",
    "    45: 'Wallflower',\n",
    "    46: 'Marigold',\n",
    "    47: 'Buttercup',\n",
    "    48: 'Oxeye Daisy',\n",
    "    49: 'Common Dandelion',\n",
    "    50: 'Petunia',\n",
    "    51: 'Wild Pansy',\n",
    "    52: 'Primula',\n",
    "    53: 'Sunflower',\n",
    "    54: 'Pelargonium',\n",
    "    55: 'Bishop of Llandaff',\n",
    "    56: 'Gaura',\n",
    "    57: 'Geranium',\n",
    "    58: 'Orange Dahlia',\n",
    "    59: 'Pink-Yellow Dahlia',\n",
    "    60: 'Cautleya Spicata',\n",
    "    61: 'Japanese Anemone',\n",
    "    62: 'Black-Eyed Susan',\n",
    "    63: 'Silverbush',\n",
    "    64: 'Californian Poppy',\n",
    "    65: 'Osteospermum',\n",
    "    66: 'Spring Crocus',\n",
    "    67: 'Bearded Iris',\n",
    "    68: 'Windflower',\n",
    "    69: 'Tree Poppy',\n",
    "    70: 'Gazania',\n",
    "    71: 'Azalea',\n",
    "    72: 'Water Lily',\n",
    "    73: 'Rose',\n",
    "    74: 'Thorn Apple',\n",
    "    75: 'Morning Glory',\n",
    "    76: 'Passion Flower',\n",
    "    77: 'Lotus',\n",
    "    78: 'Toad Lily',\n",
    "    79: 'Anthurium',\n",
    "    80: 'Frangipani',\n",
    "    81: 'Clematis',\n",
    "    82: 'Hibiscus',\n",
    "    83: 'Columbine',\n",
    "    84: 'Desert-Rose',\n",
    "    85: 'Tree Mallow',\n",
    "    86: 'Magnolia',\n",
    "    87: 'Cyclamen',\n",
    "    88: 'Watercress',\n",
    "    89: 'Canna Lily',\n",
    "    90: 'Hippeastrum',\n",
    "    91: 'Bee Balm',\n",
    "    92: 'Ball Moss',\n",
    "    93: 'Foxglove',\n",
    "    94: 'Bougainvillea',\n",
    "    95: 'Camellia',\n",
    "    96: 'Mallow',\n",
    "    97: 'Mexican Petunia',\n",
    "    98: 'Bromelia',\n",
    "    99: 'Blanket Flower',\n",
    "    100: 'Trumpet Creeper',\n",
    "    101: 'Blackberry Lily'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Distribution\n",
    "\n",
    "y = [len(generateDistribution(scipy.io.loadmat('./dataset/imagelabels.mat')['labels'][0])[x]) for x in generateDistribution(scipy.io.loadmat('./dataset/imagelabels.mat')['labels'][0]).keys()]\n",
    "plt.bar(list(range(len(y))), y)\n",
    "plt.savefig('./output/dist.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9CYuhRdVqei"
   },
   "source": [
    "## Model creation\n",
    "\n",
    "Initialisation of the model with additional layer for our custom dataset and initialisation of model optimizers and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBxKORb_VyMz"
   },
   "outputs": [],
   "source": [
    "# Importing model and setting the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torchvision.models.resnet50(weights=torchvision.models.resnet.ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "# Modifying the model to accomodate our dataset\n",
    "new_model = torch.nn.Sequential(\n",
    "    model,\n",
    "    torch.nn.Linear(1000, 786),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.Linear(786, 256),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.Linear(256, 102),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")\n",
    "model = new_model.to(device)\n",
    "\n",
    "# Importing the model optimizer and the loss functions\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=0.0001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mbEfvgsV1iC"
   },
   "source": [
    "## Training\n",
    "\n",
    "Main training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acuHQoKhV3Vy"
   },
   "outputs": [],
   "source": [
    "rawLabels = generateDistribution(scipy.io.loadmat('./dataset/imagelabels.mat')['labels'][0])\n",
    "# Training the model\n",
    "trainingDS_split, testingDS_split = generateDataSet(0.9, rawLabels)\n",
    "trainingAccuracy, validationAccuracy, validationLoss, trainingLoss = train(model, optim, loss_fn, trainingDS_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbAF0-kHV6UC"
   },
   "source": [
    "## Training plots\n",
    "\n",
    "Plotting the accuracies and losses of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEXy2UzHV9YC"
   },
   "outputs": [],
   "source": [
    "# Plotting the losses over each epoch\n",
    "plt.plot(validationLoss)\n",
    "plt.plot(trainingLoss)\n",
    "plt.legend(['Validation Loss', 'Training Loss'], loc='upper right')\n",
    "plt.savefig('./output/loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxy6T1JaWAGK"
   },
   "outputs": [],
   "source": [
    "# Plotting the accuracies over each epoch\n",
    "plt.plot(validationAccuracy)\n",
    "plt.plot(trainingAccuracy)\n",
    "plt.legend(['Validation Accuracy', 'Training Accuracy'], loc='lower right')\n",
    "plt.savefig('./output/acc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mi69UVb8WGLy"
   },
   "source": [
    "## Testing\n",
    "\n",
    "Main testing of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55lcGhE9WIO6"
   },
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "corr, total = test(model, testingDS_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUmfvgW_WNqS"
   },
   "source": [
    "## Testing plots\n",
    "\n",
    "Accuracy plots of the tested model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zT99ETzRWPnj"
   },
   "outputs": [],
   "source": [
    "# Plotting the most and least accurate flower types\n",
    "names = list(labels.values())\n",
    "plotAccuracies(corr, total, names, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yROdyzIEWYqi"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Inference of the trained model for specific input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5zqtN94WbXy"
   },
   "outputs": [],
   "source": [
    "imageID = random.randint(0, 8188)\n",
    "image = cv2.imread('./dataset/jpg/image_' + str(imageID + 1).rjust(5, '0') + '.jpg').astype(np.float32)\n",
    "# Generating the overlay based on the accuracy of the prediction\n",
    "overlay_image = np.zeros(image.shape).astype(np.float32)\n",
    "actImage_label = scipy.io.loadmat('./dataset/imagelabels.mat')['labels'][0][imageID] - 1\n",
    "predFlowerName, predImage_label = infer(image, labels)\n",
    "actFlowerName = labels[actImage_label]\n",
    "channel = 2\n",
    "symbol = '✗'\n",
    "if actImage_label == predImage_label:\n",
    "    channel = 1\n",
    "    symbol = '✓'\n",
    "overlay_image[:, :, channel] = 255.0\n",
    "image = cv2.addWeighted(image, 0.55, overlay_image, 0.45, 0).astype(np.uint8)\n",
    "plt.imshow(image[:, :, ::-1], cmap='Greens')\n",
    "plt.title(\n",
    "    f'Actual flower type: {actFlowerName}\\nPredicted flower type: {predFlowerName} {symbol}')\n",
    "plt.savefig('./output/inf.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "S8leIvotVQ4V",
    "GbAF0-kHV6UC",
    "Mi69UVb8WGLy",
    "dUmfvgW_WNqS",
    "yROdyzIEWYqi"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
